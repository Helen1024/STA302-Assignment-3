---
title: "A multiple linear model to predict the sale price of single-family, detached homes in the two neighbourhoods in the Greater Toronto Area"
author: "Linxia Li (1005715488)"
date: "December 5, 2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r, echo = FALSE}
# Import the data
a3 <- read.csv("real203.csv")
```

## I. Data Wrangling

**Report the IDs of the sample selected**

```{r, echo = FALSE}
# the seed of randomization is based on my student number
set.seed(1005715488)
data <- a3[sample(nrow(a3), 150),] # randomly select 150 cases 
data$ID # Report the IDs of the sample selected
```

**Create a new variable called "lotsize" by multiplying "lotwidth" by "lotlength" and use it to replace them**

```{r, echo = FALSE}
# Create a new variable "lotsize" and remove "lotwidth" and "lotlength"
data1 <- data %>% mutate(lotsize = lotwidth * lotlength) %>%
  select(-lotwidth, -lotlength)
glimpse(data1)
```

The above data called data1 replace two variables "lotwidth" and "lotlength" by "lotsize" which is calculated by "lotwidth" times "lotlength".

**Clean the data by removing at most eleven cases and one predictor**

```{r, echo = FALSE}
real <- data1 %>% select(-maxsqfoot) # Remove maxsqfoot
real <- na.omit(real) # Remove all rows with NA
attach(real)
glimpse(real)
```

After creating a new variable "lotsize" and using it to replace "lotwidth" and "lotlength", we obtain a new data called data1 containing 150 observations and 10 variables. From data1, we can find out that the variable "maxsqfoot" contains a number of NA which means that there are some data unavailable. We have known that NA can have an influence on the multiple linear model I would like to find. Therefore, I would like to remove one of the predictors of sale price "maxsqfoot".

Besides, the unavailable data also appear in other variables other than "maxsqfoot" so that I should remove those cases. I used "na.omit" to remove those cases and obtain a new data called real containing 141 observations and 9 variables. I removed 9 cases from the original data and the data "real" would be used in the remaining parts.


## II. Exploratory Data Analysis
 
**Classify each variable included in this assignment as categorical or discrete or continuous**

Categorical variable: location

Discrete variables: ID, sale, list, bedroom, bathroom, parking, maxsqfoot

Continuous variables: taxes, lotwidth, lotlength, lotsize

### Pairwise correlations for all pairs of quantitative variables in the data (LL5488)

```{r, echo = FALSE}
numericx = cbind(list, bedroom, bathroom, parking, taxes, lotsize)
numericxy=cbind(sale,numericx)
round(cor(numericxy), 4) # Produce the pairwise correlations
```

**Scatterplot matrix for all pairs of quantitative variables in the data**

```{r, echo = FALSE}
pairs(sale ~ list + bedroom + bathroom + parking + taxes + lotsize, data = real, cex.labels = 0.85, main = "Scatterplot matrix for all pairs of quantitative variables in the data (LL5488)") # Produce the scatterplot matrix
```

**Ranking of the quantitative predictors for sale price in terms of correlation coefficients (LL5488)**

1 | 2 | 3 | 4 | 5 | 6
-----------|:-----------:|:-----------:|:-----------:|:-----------:|:-----------|
list | taxes | bathroom | bedroom | lotsize | parking
0.9860 | 0.7751 | 0.5669 | 0.4134 | 0.3104 | 0.1569

**Description:** The above table shows the ranking of the quantitative predictors for sale price in terms of correlation coefficients from highest to lowest. The correlation coefficients are from the above pairwise correlations for all pairs of quantitative variables in the data. From the above table, we can find out that the correlation coefficients of list price for sale price is 0.9860 which is the largest among those predictors so that there is a strong linear relationship between sale price and list price. The correlation coefficients of parking for sale price is 0.1569 which is the smallest among those predictors so that there is a weak linear relationship between sale price and parking.

**A single predictor of sale price causing the assumption of constant variance be strongly violated**

Based on the scatterplot matrix, a single predictor bathroom can cause the assumption of constant variance be strongly violated. 

**A plot of the (standardized) residuals from SLR of sale price and bathroom**

```{r, echo = FALSE}
plot(lm(sale ~ bathroom, data = real), which = 3, main = "Scale-Location Plot (LL5488)") # Produce the standardized residuals plot
```

From the above Scale-Location plot of the linear regression between sale price and bathroom, we can find out that the red line is not quite approximately horizontal and the spread around the red line varies with the fitted values which means that the residuals do not appear randomly spread. Therefore, this plot confirms that the predictor bathroom can cause the assumption of constant variance be strongly violated.


## III. Methods and Model

**Fit an additive linear regression model with all available predictors variables for sale price**

```{r, echo = FALSE}
fullmodel = lm(sale ~ list + bedroom + bathroom + parking + taxes + location + lotsize, data = real)
summary(fullmodel) # Fit the fullmodel (using all available predictors for sale price)
```

**List the estimated regression coefficients and the p-values for the corresponding t-tests for these coefficients (LL5488)**

 Term | Estimate | p-value
-----------|:-----------:|:-----------:|
(intercept) | 63920 | 0.2380 
list | 0.8317 | $< 0.0001$
bedroom | 21410 | 0.1394
bathroom | 4152 | 0.7582
parking | -18980 | 0.0352
taxes | 21.91 | $< 0.0001$
locationT | 83010 | 0.0377
lotsize | -0.07245 | 0.9766 

**Interpret the estimated model coefficient if the t-test was significant**

**List:** Holding all other explanatory variables in the model fixed, for every 1 dollar increase in the list price, on average sale price increases by 0.8317 dollars.

**Parking:** Holding all other explanatory variables in the model fixed, for every 1 dollar increase in parking, on average sale price decreases by 18980
dollars.

**Taxes:** Holding all other explanatory variables in the model fixed, for every 1 dollar increase in taxes, on average sale price increases by 21.91 dollars. 

**Location:** Holding all other explanatory variables in the model fixed, the sale price for the location Toronto is 83010 larger than that for the location Mississauga. In other words, holding all other variables in the model fixed, when the location is Toronto, on average sale price increases by 83010 dollars. When the location is Mississauga, on average sale price will not increase.    

**Start with the full model fitted above and use backward elimination with AIC**

```{r, echo = FALSE}
fullmodel = lm(sale ~ list + bedroom + bathroom + parking + taxes + location + lotsize, data = real)
step(fullmodel, direction = "backward") # Using backward elimination using AIC
```

The fitted model with AIC:

$\hat{sale} = 70330 + 0.8354list + 23180bedroom - 19430parking + 21.60taxes + 78930location$

The results are not consistent with those in the fullmodel we fitted above. List price, the number of bedrooms, the total number of parking spots, taxes and location are repeated from the fullmodel as relevant predictors for sale price. However, the number of bathrooms and lotsize which were highlighted by the fullmodel were not highlighted by the model selected by backward elimination using AIC here.

**Use BIC instead of AIC**

```{r, echo = FALSE}
n = length(sale)
step(fullmodel, direction = "backward", k = log(n)) # Using backward elimination with BIC
```

The fitted model with BIC:

$\hat{sale} = 63500 + 0.8292list + 21.49taxes + 144000location$

The results are not consistent with those in the fullmodel and the model fitted with backward elimination using AIC we fitted above. List price, taxes and location are repeated from the fullmodel and the model fitted with backward elimination using AIC as relevant predictors for sale price. However, the number of bedrooms and the total number of parking spots which were highlighted by the fullmodel and the model fitted with backward elimination using AIC were not highlighted by backward elimination using BIC here.


## IV. Discussions and Limitations

**The four diagnostic plots**

```{r, echo = FALSE}
mod1 = lm(sale ~ list + taxes + location, data = real)
par(mfrow=c(2,2))
plot(mod1, main = "The diagnostic plot (LL5488)") # Producing four diagnostic plots by plotting the model obtained by backward elimination using BIC
```

**Interpret each of the four residual plots shown above**

**Residuals vs Fitted plot**

From the residual plot we made above, we can find out that the residuals are equally spread around a horizontal line which indicates that the model provides an adequate summary of the data. Besides, those residuals are correlated non-linearly with the fitted values which suggests that there is a linear trend between sale price and those three predictors: list, taxes and location.

**Normal Q-Q plot**

The normal QQ plot shows if residuals are normally distributed. We have known that if the residuals are lined well on the straight dashed line, the model we can fit is pretty good. From the normal QQ plot we made above, we can find out that most of the standardized residuals are lined well on the straight dashed line which means that the model we fitted is good and the residuals are normally distributed. Besides, we can also find out that the distribution of the response is symmetric. 

**Scale-Location plot**

The scale-location plot shows if residuals are spread equally along the ranges of predictors and can check the assumption of constant variance (homoscedasticity). From the Scale-Location plot we made above, we can find out that the red line is approximately horizontal. Besides, the spread around the red line does not vary with the fitted values which means that the residuals appear randomly spread. Therefore, it is good to see that a horizontal line with equally (randomly) spread points so that the assumption of constant variance holds.

**Residuals vs Leverage plot**

The residuals vs leverage plot can help us find the influential cases. We have known that not all outliers are influential in the linear regression analysis. Even though they have extreme values , they might not be important to determine a regression line which means that the results (model we would like to fit) would not be much different if we include or exclude them from analysis. However, some outliers can have an influence on the results. If there are some outliers at the upper right corner or at the lower right corner (outside of the dashed line, Cook's distance), the results can be influenced. However, in the residuals vs leverage plot we made, we did not find any points which are at the upper right corner or at the lower right corner (outside of the dashed line, Cook's distance) so that there are no outliers and the model we fit is good.

**Discussion about whether the normal error MLR assumptions are satisfied**

From the Residuals vs Fitted plot, the residuals are equally spread around a horizontal line without distinct patterns so that the errors are uncorrelated and there is a linear relationship between the response variable sale price and those predictors (independent variables).

From the Normal Q-Q plot, most of the standardized residuals are lined well on the straight dashed line which means that the errors are normally distributed.

From the Scale-Location Plot, the red line is approximately horizontal and the residuals appear randomly spread which means that the errors have constant variance.

Overall, the normal error MLR assumptions are satisfied.

**Next steps to find a valid final model**

In the Methods and Models section, we have used backward elimination using AIC and BIC to find a multiple linear model with the sale price as response variable and list price, taxes and location as three predictors. Besides, we also produce four diagnostic plots to show whether the model we found is good or not. However, we have not assess the predictive ability of the models we found above. Therefore, we would like to use a method called cross validation and take some steps to evaluate the predictive ability of them to find a valid final model.

We have known that k-fold Cross-validation is a standard approach to assess the predictive ability of models by evaluating their performance on a new data set.

The steps of the Cross Validation:

First, we should randomly divide the data into roughly k equal sets.

Second, we should establish the model by using all but one of the k folds and this set is called the training data set.

Third, we should use the remaining data set (the fold that was left out) called test data to evaluate the model.

Then, we should repeat the second step and the third step k times by changing the kth fold.

Eventually, we should calculate cross validation error by finding the average of the squared differences between the response and fitted values for the test set. A good candidate (model) will have small cross validation error. Therefore, the model with the smallest cross validation error would be the final model.




